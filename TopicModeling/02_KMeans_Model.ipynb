{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "## K-means Clustering\n",
    "This notebook uses k-means clustering to create topic models. We will first clean the data, then segment the reviews based on good and bad ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "reviews = json.load(open('sperryreviews.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords # ignore unimportant words like a, of, etc.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # meaningful words that occur frequently within data\n",
    "from sklearn.cluster import KMeans # k means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append('sperry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty set\n",
    "texts = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to format review data\n",
    "def load_texts(topicdata):\n",
    "    for review in topicdata:\n",
    "        if 'reviewText' in topicdata[review]:\n",
    "            reviewtext = topicdata[review]['reviewText']\n",
    "            summary = topicdata[review]['summary']\n",
    "            asin = topicdata[review]['asin']\n",
    "    \n",
    "            review = '%s %s %s' % (asin, summary, reviewtext)\n",
    "                \n",
    "            texts.add(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run function           \n",
    "load_texts(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coerce to list\n",
    "documents = list(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring stopwords into model\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "x = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish number of topics\n",
    "true_k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100000,\n",
       "    n_clusters=4, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model = KMeans(n_clusters=true_k, max_iter=100000)\n",
    "model.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster\n",
      "0: love shoes comfortable fit\n",
      "1: size big shoes shoe\n",
      "2: shoes shoe comfortable pair\n",
      "3: great shoe shoes fit\n"
     ]
    }
   ],
   "source": [
    "# find top terms per cluster\n",
    "print('Top terms per cluster')\n",
    "order_centroids = model.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[i,:4]]\n",
    "    print('%d: %s' % (i, ' '.join(topic_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully created the directory\n"
     ]
    }
   ],
   "source": [
    "# classify documents\n",
    "import os\n",
    "outputfiles = {}\n",
    "\n",
    "# initialize directory only once\n",
    "try:\n",
    "    os.mkdir('output')\n",
    "\n",
    "except OSError:\n",
    "    print('directory already exists')\n",
    "    \n",
    "else:\n",
    "    print('successfully created the directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract terms for each topic in model\n",
    "# :4 means take last four terms from variable\n",
    "for topic in range(true_k):\n",
    "    topic_terms = [terms[ind] for ind in order_centroids[topic, :4]]\n",
    "    # create empty output files for each topic\n",
    "    outputfiles[topic] = open(os.path.join('output', '_'.join(topic_terms) + '.txt'), 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine metadata with reviewdata and write to file\n",
    "for review in reviews:\n",
    "    if 'reviewText' in reviews[review]:\n",
    "        thereview = reviews[review]\n",
    "        review_str = '%s %s %s' % (thereview['asin'], thereview['summary'], thereview['reviewText'])\n",
    "        Y = vectorizer.transform([review_str])\n",
    "        \n",
    "        # reviews can have multiple classifications\n",
    "        for prediction in model.predict(Y):\n",
    "            outputfiles[prediction].write('%s\\n' % review_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close all output files            \n",
    "for n, f in outputfiles.items():\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad Reviews\n",
    "Product reviews of 2 or fewer stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "badreviews = set()\n",
    "\n",
    "def bad_review(topicdata):\n",
    "    for review in topicdata:\n",
    "        if 'reviewText' in topicdata[review]:\n",
    "            if 'overall' in topicdata[review]:\n",
    "            \n",
    "                if int(topicdata[review]['overall']) <= 2:\n",
    "                \n",
    "                    reviewtext = topicdata[review]['reviewText']\n",
    "                    summary = topicdata[review]['summary']\n",
    "                    asin = topicdata[review]['asin']\n",
    "        \n",
    "                    review = '%s %s %s' % (asin, summary, reviewtext)\n",
    "                    \n",
    "                    badreviews.add(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_review(reviews)\n",
    "documents=list(badreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "vectorizer=TfidfVectorizer(stop_words=stop_words)\n",
    "X=vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100000,\n",
       "    n_clusters=4, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model=KMeans(n_clusters=true_k,max_iter=100000)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per Cluster\n",
      "0: size small big ordered shoe fit shoes way large 10\n",
      "1: shoes shoe color return like pair would buy comfortable 34\n",
      "2: quality shoes shoe socks apart wear pair good one poor\n",
      "3: narrow wide fit shoe width shoes foot small tight back\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per Cluster\")\n",
    "\n",
    "order_centroids=model.cluster_centers_.argsort()[:,::-1]\n",
    "terms=vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    topic_terms=[terms[ind]for ind in order_centroids[i,:10]]\n",
    "    print('%d: %s' %(i,' '.join(topic_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory\n"
     ]
    }
   ],
   "source": [
    "#output of bad reviews\n",
    "import os\n",
    "outfiles={}\n",
    "\n",
    "try:\n",
    "  os.mkdir('badoutput')\n",
    "\n",
    "except OSError:\n",
    "    print ('directory already exists')\n",
    "\n",
    "else:\n",
    "    print ('Successfully created the directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data\n",
    "for atopic in range(true_k):\n",
    "    topicterms = [terms[ind] for ind in order_centroids[atopic, :4]]\n",
    "    outfiles[atopic] = open(os.path.join('badoutput', '_'.join(topicterms) + '.txt'), 'w')\n",
    "\n",
    "for areview in reviews:\n",
    "    if 'reviewText' in reviews[areview]:\n",
    "        thereview = reviews[areview]\n",
    "        reviewwithmetadata = '%s %s %s' % (thereview['asin'], thereview['summary'], thereview['reviewText'])\n",
    "        Y = vectorizer.transform([reviewwithmetadata])\n",
    "        for prediction in model.predict(Y):\n",
    "            outfiles[prediction].write('%s\\n' % reviewwithmetadata)\n",
    "            \n",
    "for n, f in outfiles.items():\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodreviews = set()\n",
    "\n",
    "def good_review(topicdata):\n",
    "    for review in topicdata:\n",
    "        if 'reviewText' in topicdata[review]:\n",
    "            if 'overall' in topicdata[review]:\n",
    "            \n",
    "            # star reviews\n",
    "                if int(topicdata[review]['overall']) >= 4:\n",
    "                \n",
    "                    reviewtext = topicdata[review]['reviewText']\n",
    "                    summary = topicdata[review]['summary']\n",
    "                    asin = topicdata[review]['asin']\n",
    "        \n",
    "                    review = '%s %s %s' % (asin, summary, reviewtext)\n",
    "                    \n",
    "                    goodreviews.add(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_review(reviews)\n",
    "documents=list(goodreviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "vectorizer=TfidfVectorizer(stop_words=stop_words)\n",
    "X=vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100000,\n",
       "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model=KMeans(n_clusters=true_k,max_iter=100000)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per Cluster\n",
      "0: shoes great shoe love comfortable\n",
      "1: size shoes shoe fit big\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per Cluster\")\n",
    "\n",
    "order_centroids=model.cluster_centers_.argsort()[:,::-1]\n",
    "terms=vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(true_k):\n",
    "    topic_terms=[terms[ind]for ind in order_centroids[i,:5]]\n",
    "    print('%d: %s' %(i,' '.join(topic_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory\n"
     ]
    }
   ],
   "source": [
    "#output of good reviews\n",
    "import os\n",
    "outfiles={}\n",
    "\n",
    "try:\n",
    "  os.mkdir('goodoutput')\n",
    "\n",
    "except OSError:\n",
    "    print ('directory already exists')\n",
    "\n",
    "else:\n",
    "    print ('Successfully created the directory')\n",
    "\n",
    "for atopic in range(true_k):\n",
    "    topicterms = [terms[ind] for ind in order_centroids[atopic, :4]]\n",
    "    outfiles[atopic] = open(os.path.join('goodoutput', '_'.join(topicterms) + '.txt'), 'w')\n",
    "\n",
    "for areview in reviews:\n",
    "    if 'reviewText' in reviews[areview]:\n",
    "        thereview = reviews[areview]\n",
    "        reviewwithmetadata = '%s %s %s' % (thereview['asin'], thereview['summary'], thereview['reviewText'])\n",
    "        Y = vectorizer.transform([reviewwithmetadata])\n",
    "        for prediction in model.predict(Y):\n",
    "            outfiles[prediction].write('%s\\n' % reviewwithmetadata)\n",
    "            \n",
    "for n, f in outfiles.items():\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
