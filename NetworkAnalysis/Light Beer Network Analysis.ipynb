{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light Beer Network Analysis\n",
    "---\n",
    "### APRD 6346\n",
    "### Author: Matt Hardwick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs network analysis on tweets involving the competiting light beer brands of *Bud Light*, *Coors Light*, and *Miller Light*. The first set of networks are Mention Networks that analyze which Twitter users are tweeting at which brand. The second set of networks are Semantic Networks that associate specific words from tweets with their respective brands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "import nltk # text processor\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "ps = nltk.PorterStemmer()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from time import sleep\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary directory to extract zip files to\n",
    "TMPDIR = 'tmp'\n",
    "\n",
    "if not os.path.exists(TMPDIR):\n",
    "    os.makedirs(TMPDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['miller_lite_OR_millerlite.zip',\n",
       " 'bud_light_OR_budlight.zip',\n",
       " 'coors_light_OR_coorslight.zip']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetzipfiles = glob.glob('*.zip')\n",
    "tweetzipfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping to tmp directory: miller_lite_OR_millerlite.zip\n",
      "Unzipping to tmp directory: bud_light_OR_budlight.zip\n",
      "Unzipping to tmp directory: coors_light_OR_coorslight.zip\n"
     ]
    }
   ],
   "source": [
    "# extract all tweets to temp directory\n",
    "for tweetzipfile in tweetzipfiles:\n",
    "    with zipfile.ZipFile(tweetzipfile, 'r') as f:\n",
    "        print('Unzipping to tmp directory: %s' % tweetzipfile)\n",
    "        f.extractall(TMPDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mention Network\n",
    "---\n",
    "The Mention Network will display the users who are tweeting at each light beer brand. Because there are so many tweets, we will filter just the users who have tweeted about a brand multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets opened: 1000\n",
      "Tweets opened: 2000\n",
      "Tweets opened: 3000\n",
      "Tweets opened: 4000\n",
      "Tweets opened: 5000\n",
      "Tweets opened: 6000\n",
      "Tweets opened: 7000\n",
      "Tweets opened: 8000\n",
      "Tweets opened: 9000\n",
      "Tweets opened: 10000\n"
     ]
    }
   ],
   "source": [
    "# create dictionary with profile names and number of tweets\n",
    "uniqueusers = {}\n",
    "count = 0\n",
    "\n",
    "for fn in os.listdir(TMPDIR):\n",
    "    fn = os.path.join(TMPDIR, fn)\n",
    "    with open(fn) as f:\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(\"Tweets opened: %s\" % count)\n",
    "            \n",
    "        tweetjson = json.load(f)\n",
    "        userwhotweeted = tweetjson['user']['screen_name']\n",
    "        \n",
    "        if userwhotweeted in uniqueusers:\n",
    "            uniqueusers[userwhotweeted] += 1\n",
    "        if userwhotweeted not in uniqueusers:\n",
    "            uniqueusers[userwhotweeted] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9130"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniqueusers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662\n"
     ]
    }
   ],
   "source": [
    "# users who tweet more than once\n",
    "userstoinclude = set()\n",
    "usercount = 0\n",
    "\n",
    "for user in uniqueusers:\n",
    "    if uniqueusers[user] > 1:\n",
    "        usercount += 1\n",
    "        userstoinclude.add(user)\n",
    "        \n",
    "print(len(userstoinclude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a network in Gephi, the data needs to be formatted as an edge list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing edge list csv\n",
    "edgelist = open('lightbeer.mention.full.gephi.csv', 'w')\n",
    "csvwriter = csv.writer(edgelist)\n",
    "header = ['Source', 'Target']\n",
    "csvwriter.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing edge list\n",
      "Tweets written: 1000\n",
      "Tweets written: 2000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "print('Writing edge list')\n",
    "\n",
    "for fn in os.listdir(TMPDIR):\n",
    "    fn = os.path.join(TMPDIR, fn)\n",
    "    with open(fn) as f:\n",
    "        tweetjson = json.load(f)\n",
    "        userwhotweeted = tweetjson['user']['screen_name']\n",
    "        if userwhotweeted in userstoinclude:\n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print(\"Tweets written: %s\" % count)\n",
    "                \n",
    "            users = tweetjson['entities']['user_mentions']\n",
    "            if len(users) > 0:\n",
    "                for user in users:\n",
    "                    screenname = user['screen_name']\n",
    "                    row = [userwhotweeted, screenname]\n",
    "                    csvwriter.writerow(row)\n",
    "\n",
    "edgelist.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Mention Full.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Users Per Brand\n",
    "\n",
    "*Coors Light*: marketingdive, sullnmika, Doritos, MKEBizJournal, CarpeZytha, SLeskMBJ, lewbryson, oldmudgie, nwi_jsp, jamesbwxm\n",
    "\n",
    "The main users tweeting @CoorsLight are beer reviewers, pubs, and journalists. A good marketing strategy would be to supply these users with beer samples and merchandise so they may promote Coors products. A promotion with Doritos could also improve sales, as beer and chips are commonly consumed together.\n",
    "\n",
    "*Bud Light*: rkbennet, Tyne_Ag, MillerCoors, Cornfrmr, NationalCorn, paulwhittington, BlueJacketsNHL, CarpeZytha, lewbryson, MKEBizJournal\n",
    "\n",
    "The main users tweeting @budlight are farmers and beer reviewers. Advertising *Bud Light* to regions with dense agricultural populations could lead to increased sales. Many of the reviewers also reviewed *Coors Light*, so Budweiser is competing for their mentions. A promotion with the Colombus Blue Jackets would appeal to sports fans and may even reach other NHL teams.\n",
    "\n",
    "*Miller Lite*: AdamDCollins, dennismonsewicz, M2Third, KCLiveBlock, csbev, Cornlrmr, ComClassic, mattcham37, vogeliowa, Morning_Ag\n",
    "\n",
    "The main users tweeting @MillerLite are farmers and beer distributors. A marketing strategy similar to the *Bud Light* recommendation above would improve sales to farmers, and would lead to increased competition. Interestingly, MillerCoors bridges *Bud Light* and *Miller Light* together. This account can be promoted to improve Miller's image while hurting Bud's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verified Users\n",
    "We can perform the same network analysis on verified Twitter users. These users generally have more social media influence than unverified users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets opened: 1000\n",
      "Tweets opened: 2000\n",
      "Tweets opened: 3000\n",
      "Tweets opened: 4000\n",
      "Tweets opened: 5000\n",
      "Tweets opened: 6000\n",
      "Tweets opened: 7000\n",
      "Tweets opened: 8000\n",
      "Tweets opened: 9000\n",
      "Tweets opened: 10000\n"
     ]
    }
   ],
   "source": [
    "verifiedusers = {}\n",
    "count = 0\n",
    "\n",
    "for fn in os.listdir(TMPDIR):\n",
    "    fn = os.path.join(TMPDIR, fn)\n",
    "    with open(fn) as f:\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(\"Tweets opened: %s\" % count)\n",
    "        tweetjson = json.load(f)\n",
    "        userwhotweeted = tweetjson['user']['screen_name']\n",
    "\n",
    "        verified = tweetjson['user']['verified']\n",
    "        if verified == True:\n",
    "        \n",
    "            if userwhotweeted in verifiedusers:\n",
    "                verifiedusers[userwhotweeted] += 1\n",
    "            if userwhotweeted not in verifiedusers:\n",
    "                verifiedusers[userwhotweeted] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(verifiedusers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing edge list csv\n",
    "edgelist = open('lightbeer.mention.verified.gephi.csv', 'w')\n",
    "csvwriter = csv.writer(edgelist)\n",
    "header = ['Source', 'Target']\n",
    "csvwriter.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing edge list\n"
     ]
    }
   ],
   "source": [
    "print('Writing edge list')\n",
    "\n",
    "for fn in os.listdir(TMPDIR):\n",
    "    fn = os.path.join(TMPDIR, fn)\n",
    "    with open(fn) as f:\n",
    "        tweetjson = json.load(f)\n",
    "        userwhotweeted = tweetjson['user']['screen_name']\n",
    "        if userwhotweeted in verifiedusers:\n",
    "            users = tweetjson['entities']['user_mentions']\n",
    "            if len(users) > 0:\n",
    "                for user in users:\n",
    "                    screenname = user['screen_name']\n",
    "                    row = [userwhotweeted, screenname]\n",
    "                    csvwriter.writerow(row)\n",
    "\n",
    "edgelist.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Mention Verified.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Verified Users Per Brand\n",
    "\n",
    "*Coors Light*: kentjlewis, iDarija, NBCSPhilly, AdAgeIn, beerbabe, TheDrum, DuffersTavern, lewbryson\n",
    "\n",
    "Marketers, beer writers, and pubs again dominate *Coors Light* mentions. Coors will compete with Bud over writers and marketing agencies, but NBC Sports Philly offers opportunities for good promotions. Coors can improve its sales by marketing to sports fans on a major news platform such as NBC.\n",
    "\n",
    "*Bud Light*: AdAgeIn, beerbabe, rkbennet, paulwhittington, AmerksHockey, BlueJacketsNHL, peterfrost, MillerCoors\n",
    "\n",
    "Again, *Bud Light* cements itself between Coors and Miller, drawing competition from their mutual tweeters. Marketers, beer writers, and hockey teams are tweeting at Bud the most. Sending swag to the writers and promoting with hockey teams can help improve sales.\n",
    "\n",
    "*Miller Lite*: SyracuseCrunch, Molson_Canadian, Stephens2727, MillerCoors, Beer_Notes, peterfrost\n",
    "\n",
    "Hockey and writers are the two themes tweeting at *Miller Lite*. Miller can compete with Bud over the hockey market and its fans. MillerCoors again bridges *Bud Light* with *Miller Light*, and its platform can help advertise Miller and boost its market share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retweets and Favorites\n",
    "Many Tweets go ignored, so filtering Tweets with at least one retweet and one favorite can remove users whose Tweets are less relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets opened: 1000\n",
      "Tweets opened: 2000\n",
      "Tweets opened: 3000\n",
      "Tweets opened: 4000\n",
      "Tweets opened: 5000\n",
      "Tweets opened: 6000\n",
      "Tweets opened: 7000\n",
      "Tweets opened: 8000\n",
      "Tweets opened: 9000\n",
      "Tweets opened: 10000\n"
     ]
    }
   ],
   "source": [
    "rtusers = {}\n",
    "count = 0\n",
    "\n",
    "for fn in os.listdir(TMPDIR):\n",
    "    fn = os.path.join(TMPDIR, fn)\n",
    "    #print(fn)\n",
    "    with open(fn) as f:\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(\"Tweets opened: %s\" % count)\n",
    "        tweetjson = json.load(f)\n",
    "        userwhotweeted = tweetjson['user']['screen_name']\n",
    "        \n",
    "        rtcount = tweetjson['retweet_count']\n",
    "        favcount = tweetjson['favorite_count']\n",
    "        if rtcount > 1 and favcount > 1:\n",
    "        \n",
    "            if userwhotweeted in rtusers:\n",
    "                rtusers[userwhotweeted] += 1\n",
    "            if userwhotweeted not in rtusers:\n",
    "                rtusers[userwhotweeted] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rtusers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing edge list csv\n",
    "edgelist = open('lightbeer.mention.rtfav.gephi.csv', 'w')\n",
    "csvwriter = csv.writer(edgelist)\n",
    "header = ['Source', 'Target']\n",
    "csvwriter.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing edge list\n"
     ]
    }
   ],
   "source": [
    "print('Writing edge list')\n",
    "\n",
    "for fn in os.listdir(TMPDIR):\n",
    "    fn = os.path.join(TMPDIR, fn)\n",
    "    with open(fn) as f:\n",
    "        tweetjson = json.load(f)\n",
    "        userwhotweeted = tweetjson['user']['screen_name']\n",
    "        if userwhotweeted in rtusers:\n",
    "            users = tweetjson['entities']['user_mentions']\n",
    "            if len(users) > 0:\n",
    "                for user in users:\n",
    "                    screenname = user['screen_name']\n",
    "                    row = [userwhotweeted, screenname]\n",
    "                    csvwriter.writerow(row)\n",
    "\n",
    "edgelist.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Mention RTfav.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Retweeted and Favorited Users Per Brand\n",
    "\n",
    "*Coors Light*: lewbryson, WhiskeyRiff, andimariieee, JobmanAg\n",
    "\n",
    "Country is the theme for Coors. The beer writer @lewbryson appears again, but the other three users enjoy the country lifestyle. Coors does a great job of marketing itself to southern states and the country market, and it should continue to do so to maximize its reach.\n",
    "\n",
    "*Bud Light*: AmerksHockey, BlueJacketsNHL, lewbryson, kscornfed1, Cornfrmr, Tyne_Ag, MillerCoors\n",
    "\n",
    "Hockey clubs, farmers, and writers are the three groups of Twitter users that interact with *Bud Light* the most across all three mention networks. Partnerships and promotions catered to these groups will help Bud reach its largest audience.\n",
    "\n",
    "*Miller Lite*: AdamDCollins, csbev, BroughtTheDog, Cornfrmr, hopnotes\n",
    "\n",
    "*Miller Lite* is all over the board, with users ranging from farmers to writers to distributors to a rock band. Rather than competing with Bud, Miller should focus its marketing efforts to rock groups and the music scene. No other bands have appeared in the mention networks, so Miller has an opportunity to reach a new Twitter market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Network\n",
    "---\n",
    "The semantic network will display key words from Tweets associated with each brand. Rather than showing who is talking about each brand, this network shows what people are saying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up stopwords\n",
    "punctuation = string.punctuation\n",
    "stopwordsset = set(stopwords.words(\"english\"))\n",
    "stopwordsset.add(\"'s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing urls\n",
    "def removeURL(text):\n",
    "    result = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return result\n",
    "\n",
    "#Extracting contextual words from a sentence\n",
    "def tokenize(text):\n",
    "    #lower case\n",
    "    text = text.lower()\n",
    "    #split into individual words\n",
    "    words = word_tokenize(text)\n",
    "    return words\n",
    "\n",
    "def stem(tokenizedtext):\n",
    "    rootwords = []\n",
    "    for aword in tokenizedtext:\n",
    "        aword = ps.stem(aword)\n",
    "        rootwords.append(aword)\n",
    "    return rootwords\n",
    "\n",
    "def stopWords(tokenizedtext):\n",
    "    goodwords = []\n",
    "    for aword in tokenizedtext:\n",
    "        if aword not in stopwordsset:\n",
    "            goodwords.append(aword)\n",
    "    return goodwords\n",
    "\n",
    "def lemmatizer(tokenizedtext):\n",
    "    lemmawords = []\n",
    "    for aword in tokenizedtext:\n",
    "        aword = wn.lemmatize(aword)\n",
    "        lemmawords.append(aword)\n",
    "    return lemmawords\n",
    "\n",
    "def removePunctuation(tokenizedtext):\n",
    "    nopunctwords = []\n",
    "    for aword in tokenizedtext:\n",
    "        if aword not in punctuation:\n",
    "            nopunctwords.append(aword)\n",
    "    cleanedwords = []\n",
    "    for aword in nopunctwords:\n",
    "        aword = aword.translate(str.maketrans('', '', string.punctuation))\n",
    "        cleanedwords.append(aword)\n",
    "    return cleanedwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets parsed: 1000\n",
      "Tweets parsed: 2000\n",
      "Tweets parsed: 3000\n",
      "Tweets parsed: 4000\n",
      "Tweets parsed: 5000\n",
      "Tweets parsed: 6000\n",
      "Tweets parsed: 7000\n",
      "Tweets parsed: 8000\n",
      "Tweets parsed: 9000\n",
      "Tweets parsed: 10000\n"
     ]
    }
   ],
   "source": [
    "# create dictionary with words and weights\n",
    "uniquewords = {}\n",
    "count = 0\n",
    "\n",
    "for fn in os.listdir(TMPDIR):\n",
    "    fn = os.path.join(TMPDIR, fn)\n",
    "    with open(fn) as f:\n",
    "        tweetjson = json.load(f)\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(\"Tweets parsed: %s\" % count)\n",
    "            \n",
    "        text = tweetjson['text']\n",
    "        nourlstext = removeURL(text) #remove url\n",
    "        tokenizedtext = tokenize(nourlstext) #separate text by each word\n",
    "        nostopwordstext = stopWords(tokenizedtext) #remove irrelevant words\n",
    "        lemmatizedtext = lemmatizer(nostopwordstext) #root words\n",
    "        nopuncttext = removePunctuation(lemmatizedtext) #remove punctuation\n",
    "\n",
    "        for aword in nopuncttext:\n",
    "            if aword in uniquewords:\n",
    "                uniquewords[aword] += 1\n",
    "            if aword not in uniquewords:\n",
    "                uniquewords[aword] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687\n"
     ]
    }
   ],
   "source": [
    "# filter words repeated at least 25 times\n",
    "wordstoinclude = set()\n",
    "wordcount = 0\n",
    "\n",
    "for aword in uniquewords:\n",
    "    if uniquewords[aword] > 25:\n",
    "        wordcount += 1\n",
    "        wordstoinclude.add(aword)\n",
    "    \n",
    "print(wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Edge List\n",
      "Tweets parsed: 1000\n",
      "Tweets parsed: 2000\n",
      "Tweets parsed: 3000\n",
      "Tweets parsed: 4000\n",
      "Tweets parsed: 5000\n",
      "Tweets parsed: 6000\n",
      "Tweets parsed: 7000\n",
      "Tweets parsed: 8000\n",
      "Tweets parsed: 9000\n",
      "Tweets parsed: 10000\n"
     ]
    }
   ],
   "source": [
    "edgelist = open('lightbeer.semantic.full.gephi.csv', 'w')\n",
    "csvwriter = csv.writer(edgelist)\n",
    "header = ['Source', 'Target', 'Type'] #undirected network\n",
    "csvwriter.writerow(header)\n",
    "\n",
    "print(\"Writing Edge List\")\n",
    "\n",
    "uniquewords = {}\n",
    "count = 0\n",
    "\n",
    "for fn in os.listdir(TMPDIR):\n",
    "    fn = os.path.join(TMPDIR, fn)\n",
    "    with open(fn) as f:\n",
    "        tweetjson = json.load(f)\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(\"Tweets parsed: %s\" % count)\n",
    "            \n",
    "        text = tweetjson['text']\n",
    "        nourlstext = removeURL(text) #remove url\n",
    "        tokenizedtext = tokenize(nourlstext) #separate text by each word\n",
    "        nostopwordstext = stopWords(tokenizedtext) #remove irrelevant words\n",
    "        lemmatizedtext = lemmatizer(nostopwordstext) #root words\n",
    "        nopuncttext = removePunctuation(lemmatizedtext) #remove punctuation\n",
    "        \n",
    "        goodwords = []\n",
    "        for aword in nopuncttext:\n",
    "            if aword in wordstoinclude:\n",
    "                goodwords.append(aword.replace(',',''))\n",
    "        \n",
    "        allcombos = itertools.combinations(goodwords, 2)\n",
    "        for acombo in allcombos:\n",
    "            row = []\n",
    "            for anode in acombo:\n",
    "                row.append(anode)\n",
    "            row.append('Undirected')\n",
    "            csvwriter.writerow(row)\n",
    "            \n",
    "edgelist.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Semantic Full.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Words Per Brand\n",
    "\n",
    "*Coors Light*: another, hard, use, thought, ad, im, commercial, syrup, put, remember, ve, bowl\n",
    "\n",
    "*Bud Light*: white, always, better, think, say, good, right, make, last, cold, fan, college\n",
    "\n",
    "*Miller Lite*: anheuserbusch, summer, millercoors, away, way, mean, orange, top\n",
    "\n",
    "Unfortunately, this basic semantic network isn't very intuitive. At a glance, *Coors Light* is commercialized, *Bud Light* is good for college fans, and *Miller Lite* is a summer beer. If that is how each brand wants to be perceived, then our work here is done. However, each light beer brand is competing to be the tastiest for all occasions. Adding filters to the Tweets can create more intuitive networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "Using the TextBlob package we can assign each Tweet a sentiment score. Modeling semantic networks for both positively and negatively scored words can provide insights on how each brand is perceived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive sentiment\n",
    "positivewords = {}\n",
    "count = 0\n",
    "\n",
    "for fn in os.listdir(TMPDIR):\n",
    "    fn = os.path.join(TMPDIR, fn)\n",
    "    with open(fn) as f:\n",
    "        tweetjson = json.load(f)\n",
    "\n",
    "        text = tweetjson['text']\n",
    "        \n",
    "        sentimentscore = TextBlob(text)\n",
    "        score = sentimentscore.sentiment.polarity\n",
    "        \n",
    "        if score > 0:\n",
    "        \n",
    "            nourlstext = removeURL(text) #remove url\n",
    "            tokenizedtext = tokenize(nourlstext) #separate text by each word\n",
    "            nostopwordstext = stopWords(tokenizedtext) #remove irrelevant words\n",
    "            lemmatizedtext = lemmatizer(nostopwordstext) #root words\n",
    "            nopuncttext = removePunctuation(lemmatizedtext) #remove punctuation\n",
    "\n",
    "            for aword in nopuncttext:\n",
    "                if aword in positivewords:\n",
    "                    positivewords[aword] += 1\n",
    "                if aword not in positivewords:\n",
    "                    positivewords[aword] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "positivewordstoinclude = set()\n",
    "wordcount = 0\n",
    "\n",
    "for aword in positivewords:\n",
    "    if positivewords[aword] > 25:\n",
    "        wordcount += 1\n",
    "        positivewordstoinclude.add(aword)\n",
    "    \n",
    "print(wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Edge List\n"
     ]
    }
   ],
   "source": [
    "edgelist = open('lightbeer.semantic.positive.gephi.csv', 'w')\n",
    "csvwriter = csv.writer(edgelist)\n",
    "header = ['Source', 'Target', 'Type'] #undirected network\n",
    "csvwriter.writerow(header)\n",
    "\n",
    "print(\"Writing Edge List\")\n",
    "\n",
    "positivewords = {}\n",
    "count = 0\n",
    "\n",
    "for fn in os.listdir(TMPDIR):\n",
    "    fn = os.path.join(TMPDIR, fn)\n",
    "    with open(fn) as f:\n",
    "        tweetjson = json.load(f)\n",
    "            \n",
    "        text = tweetjson['text']\n",
    "        \n",
    "        sentimentscore = TextBlob(text)\n",
    "        score = sentimentscore.sentiment.polarity\n",
    "        \n",
    "        if score > 0:\n",
    "    \n",
    "            nourlstext = removeURL(text) #remove url\n",
    "            tokenizedtext = tokenize(nourlstext) #separate text by each word\n",
    "            nostopwordstext = stopWords(tokenizedtext) #remove irrelevant words\n",
    "            lemmatizedtext = lemmatizer(nostopwordstext) #root words\n",
    "            nopuncttext = removePunctuation(lemmatizedtext) #remove punctuation\n",
    "\n",
    "            goodwords = []\n",
    "            for aword in nopuncttext:\n",
    "                if aword in positivewordstoinclude:\n",
    "                    goodwords.append(aword.replace(',',''))\n",
    "\n",
    "            allcombos = itertools.combinations(goodwords, 2)\n",
    "            for acombo in allcombos:\n",
    "                row = []\n",
    "                for anode in acombo:\n",
    "                    row.append(anode)\n",
    "                row.append('Undirected')\n",
    "                csvwriter.writerow(row)\n",
    "            \n",
    "edgelist.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Semantic Positive.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Positive Words Per Brand\n",
    "\n",
    "*Coors Light*: bowl, think, look, said, even, might, anyone, brand, someone, corn, commercial\n",
    "\n",
    "*Bud Light*: new, right, love, first, super, going, guy, everyone, lime, good, time, game\n",
    "\n",
    "*Miller Lite*: thank, sale, nice, lot, tap, ipa, sound, craft\n",
    "\n",
    "Each of these nodes stem from what TextBlob considers to be positive Tweets. *Coors Light* and *Bud Light* are close together and compete for some of the words. Both seem to have received attention from Super Bowl ads, but Coors is still more commercialized. On the other hand, Bud appeals to everyone.\n",
    "\n",
    "*Miller Lite* stands away from the competition at the bottom of the network. It's perceived as a nicer beer than Coors and Bud. Miller should reinforce this perception and market to drinkers who want a light beer that isn't as low-end as *Coors Light* or *Bud Light*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative sentiment\n",
    "negativewords = {}\n",
    "count = 0\n",
    "\n",
    "for fn in os.listdir(TMPDIR):\n",
    "    fn = os.path.join(TMPDIR, fn)\n",
    "    with open(fn) as f:\n",
    "        tweetjson = json.load(f)\n",
    "        \n",
    "        text = tweetjson['text']\n",
    "        \n",
    "        sentimentscore = TextBlob(text)\n",
    "        score = sentimentscore.sentiment.polarity\n",
    "        \n",
    "        if score < 0:\n",
    "        \n",
    "            nourlstext = removeURL(text) #remove url\n",
    "            tokenizedtext = tokenize(nourlstext) #separate text by each word\n",
    "            nostopwordstext = stopWords(tokenizedtext) #remove irrelevant words\n",
    "            lemmatizedtext = lemmatizer(nostopwordstext) #root words\n",
    "            nopuncttext = removePunctuation(lemmatizedtext) #remove punctuation\n",
    "\n",
    "            for aword in nopuncttext:\n",
    "                if aword in negativewords:\n",
    "                    negativewords[aword] += 1\n",
    "                if aword not in negativewords:\n",
    "                    negativewords[aword] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "negativewordstoinclude = set()\n",
    "wordcount = 0\n",
    "\n",
    "for aword in negativewords:\n",
    "    if negativewords[aword] > 25:\n",
    "        wordcount += 1\n",
    "        negativewordstoinclude.add(aword)\n",
    "    \n",
    "print(wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Edge List\n"
     ]
    }
   ],
   "source": [
    "edgelist = open('lightbeer.semantic.negative.gephi.csv', 'w')\n",
    "csvwriter = csv.writer(edgelist)\n",
    "header = ['Source', 'Target', 'Type'] #undirected network\n",
    "csvwriter.writerow(header)\n",
    "\n",
    "print(\"Writing Edge List\")\n",
    "\n",
    "negativewords = {}\n",
    "count = 0\n",
    "\n",
    "for fn in os.listdir(TMPDIR):\n",
    "    fn = os.path.join(TMPDIR, fn)\n",
    "    with open(fn) as f:\n",
    "        tweetjson = json.load(f)\n",
    "            \n",
    "        text = tweetjson['text']\n",
    "        \n",
    "        sentimentscore = TextBlob(text)\n",
    "        score = sentimentscore.sentiment.polarity\n",
    "        \n",
    "        if score < 0:\n",
    "    \n",
    "            nourlstext = removeURL(text) #remove url\n",
    "            tokenizedtext = tokenize(nourlstext) #separate text by each word\n",
    "            nostopwordstext = stopWords(tokenizedtext) #remove irrelevant words\n",
    "            lemmatizedtext = lemmatizer(nostopwordstext) #root words\n",
    "            nopuncttext = removePunctuation(lemmatizedtext) #remove punctuation\n",
    "\n",
    "            goodwords = []\n",
    "            for aword in nopuncttext:\n",
    "                if aword in negativewordstoinclude:\n",
    "                    goodwords.append(aword.replace(',',''))\n",
    "\n",
    "            allcombos = itertools.combinations(goodwords, 2)\n",
    "            for acombo in allcombos:\n",
    "                row = []\n",
    "                for anode in acombo:\n",
    "                    row.append(anode)\n",
    "                row.append('Undirected')\n",
    "                csvwriter.writerow(row)\n",
    "            \n",
    "edgelist.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Semantic Negative.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Negative Words Per Brand\n",
    "\n",
    "*Coors*: get, shit, know, need, right, crack, drunk\n",
    "\n",
    "*Bud*: light, come, night, got, game\n",
    "\n",
    "*Miller*: drink, lite, amp, cold\n",
    "\n",
    "TextBlob only rated a handful of Tweets to be negative, so it's difficult to truly gain any insights from this network. Additionally, the full Twitter handles displayed in previous networks were filtered out. What remains are the shortened names for each beer brand. *Coors* seems to get the worst of the negative words, but its difficult to tell what users negatively think about *Bud* and *Miller*.\n",
    "\n",
    "A better method of modeling negative sentiment is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Words\n",
    "Often, light beers are considered to be low-quality and taste like urine. To find out which light beer tastes the most like pee, we can create a list of specific words to be modeled in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of words to match\n",
    "specificwords = ['pee', 'piss', 'urine']\n",
    "\n",
    "uniquewords = {}\n",
    "count = 0\n",
    "\n",
    "for fn in os.listdir(TMPDIR):\n",
    "    fn = os.path.join(TMPDIR, fn)\n",
    "    with open(fn) as f:\n",
    "        tweetjson = json.load(f)\n",
    "            \n",
    "        text = tweetjson['text']\n",
    "        \n",
    "        # matching specific words list\n",
    "        match = 0\n",
    "        for word in specificwords:\n",
    "            if word in text:\n",
    "                match += 1\n",
    "\n",
    "        if match > 0:\n",
    "        \n",
    "            nourlstext = removeURL(text) #remove url\n",
    "            tokenizedtext = tokenize(nourlstext) #separate text by each word\n",
    "            nostopwordstext = stopWords(tokenizedtext) #remove irrelevant words\n",
    "            lemmatizedtext = lemmatizer(nostopwordstext) #root words\n",
    "            nopuncttext = removePunctuation(lemmatizedtext) #remove punctuation\n",
    "\n",
    "            for aword in nopuncttext:\n",
    "                if aword in uniquewords:\n",
    "                    uniquewords[aword] += 1\n",
    "                if aword not in uniquewords:\n",
    "                    uniquewords[aword] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "wordstoinclude = set()\n",
    "wordcount = 0\n",
    "\n",
    "for aword in uniquewords:\n",
    "    if uniquewords[aword] > 1:\n",
    "        wordcount += 1\n",
    "        wordstoinclude.add(aword)\n",
    "    \n",
    "print(wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Edge List\n"
     ]
    }
   ],
   "source": [
    "edgelist = open('lightbeer.semantic.piss.gephi.csv', 'w')\n",
    "csvwriter = csv.writer(edgelist)\n",
    "header = ['Source', 'Target', 'Type'] #undirected network\n",
    "csvwriter.writerow(header)\n",
    "\n",
    "print(\"Writing Edge List\")\n",
    "\n",
    "uniquewords = {}\n",
    "count = 0\n",
    "\n",
    "for fn in os.listdir(TMPDIR):\n",
    "    fn = os.path.join(TMPDIR, fn)\n",
    "    with open(fn) as f:\n",
    "        tweetjson = json.load(f)\n",
    "            \n",
    "        text = tweetjson['text']\n",
    "        nourlstext = removeURL(text) #remove url\n",
    "        tokenizedtext = tokenize(nourlstext) #separate text by each word\n",
    "        nostopwordstext = stopWords(tokenizedtext) #remove irrelevant words\n",
    "        lemmatizedtext = lemmatizer(nostopwordstext) #root words\n",
    "        nopuncttext = removePunctuation(lemmatizedtext) #remove punctuation\n",
    "        \n",
    "        goodwords = []\n",
    "        for aword in nopuncttext:\n",
    "            if aword in wordstoinclude:\n",
    "                goodwords.append(aword.replace(',',''))\n",
    "        \n",
    "        allcombos = itertools.combinations(goodwords, 2)\n",
    "        for acombo in allcombos:\n",
    "            row = []\n",
    "            for anode in acombo:\n",
    "                row.append(anode)\n",
    "            row.append('Undirected')\n",
    "            csvwriter.writerow(row)\n",
    "            \n",
    "edgelist.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/Semantic Piss.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Urinary Words Per Brand\n",
    "\n",
    "*Coors*: taste, amp, water, horse, natty, pbr, weekend, garbage\n",
    "\n",
    "*Bud*: piss, drinking, see, get, well, bar, cold\n",
    "\n",
    "*Miler*: make, would, people, bottle, good\n",
    "\n",
    "*Coors* takes home the trophy for being the worst light beer. Although \"piss\" is slightly closer to *Bud*, *Coors* is being compared to other low quality beers and is closest to \"pee\". Again, *Miller* separates itself from its competitors by being thought of as higher quality. *Coors* needs to work to improve its image, while *Miller* should market itself as a premium light beer.\n",
    "\n",
    "To no suprise, the bridging words between the three brands are \"drink\", \"light\", \"lite\", and \"beer\". This leads to no market insights, but it shows there is a fairly even distribution of Tweets amongst the three brands."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
